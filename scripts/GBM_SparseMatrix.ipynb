{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import utils\n",
    "import utils_fe\n",
    "from gbm_pipeline import GBMPipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_lgb = False\n",
    "run_kfold = True\n",
    "split_for_validation = True\n",
    "\n",
    "if run_kfold:\n",
    "    prefix = 'KFold'\n",
    "else:\n",
    "    prefix = 'Bag'\n",
    "\n",
    "    \n",
    "run_name = 'XGB_{}_TfidfBasic'.format(prefix)\n",
    "src = '/home/w/Projects/Toxic/data/features/'\n",
    "\n",
    "\n",
    "train = pd.read_pickle(\"../data/train_basic_clean.pkl\")\n",
    "target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "X = pd.read_pickle(\"../data/features/data_Tfidf_2GramWord.pkl\")\n",
    "\n",
    "X_train = X[:train.shape[0], :]\n",
    "X_test = X[train.shape[0]:, :]\n",
    "\n",
    "\n",
    "del X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train = X_train.iloc[:train.shape[0], :][:1000]\n",
    "train = train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 20,\n",
    "    'subsample': 0.8,\n",
    "    'lambda': 0,\n",
    "    'tree_method': 'hist',\n",
    "    'nthread': 4,\n",
    "    'silent': True,\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 255,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 20,\n",
    "    'subsample': 0.8,\n",
    "    'reg_lambda': 0,\n",
    "    'nthread': 10,\n",
    "}\n",
    "\n",
    "\n",
    "if split_for_validation:\n",
    "    \n",
    "    train_params = {\n",
    "        'boost_round': 10000,\n",
    "        'stopping_rounds': 50,\n",
    "        'verbose_eval': 50,\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    \n",
    "    train_params = {\n",
    "        'boost_round': 161,\n",
    "        'stopping_rounds': 50,\n",
    "        'verbose_eval': False,\n",
    "    }\n",
    "\n",
    "    \n",
    "pipeline_params = {\n",
    "    'use_lgb': use_lgb,\n",
    "    'predict_test': True,\n",
    "    'seed': 1337,\n",
    "    'shuffle': True,\n",
    "    'verbose': True,\n",
    "    'run_save_name': run_name,\n",
    "    'save_model': False,\n",
    "    'save_history': False,\n",
    "    'save_statistics': False,\n",
    "    'output_statistics': True,\n",
    "    'output_importance': True,\n",
    "}\n",
    "\n",
    "\n",
    "XGB_pipeline = GBMPipeline(\n",
    "    use_lgb=pipeline_params['use_lgb'],\n",
    "    predict_test=pipeline_params['predict_test'],\n",
    "    seed=pipeline_params['seed'],\n",
    "    shuffle=pipeline_params['shuffle'],\n",
    "    verbose=pipeline_params['verbose'],\n",
    "    run_save_name=pipeline_params['run_save_name'],\n",
    "    save_model=pipeline_params['save_model'],\n",
    "    save_history=pipeline_params['save_history'],\n",
    "    save_statistics=pipeline_params['save_statistics'],\n",
    "    output_statistics=pipeline_params['output_statistics'],\n",
    "    output_importance=pipeline_params['output_importance'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using XGBoost\n",
      "Running bagging (currently just one bag).\n",
      "Start training with parameters: {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'eta': 0.05, 'max_depth': 10, 'min_child_weight': 20, 'subsample': 0.8, 'lambda': 0, 'tree_method': 'hist', 'nthread': 4, 'silent': True} \n",
      " \n",
      "\n",
      "X_train shape: (95851, 668367)\n",
      "X_test shape: (226998, 668367)\n",
      "Splitting data - validation split size: 0.2, split seed: 1337\n",
      "Training model for column: toxic\n",
      "[0]\ttrain-logloss:0.654975\tvalid-logloss:0.655032\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.199078\tvalid-logloss:0.202288\n"
     ]
    }
   ],
   "source": [
    "if pipeline_params['use_lgb']:\n",
    "    gbm_params = lgb_params\n",
    "else:\n",
    "    gbm_params = xgb_params\n",
    "\n",
    "\n",
    "if pipeline_params['predict_test']:\n",
    "    val_preds, test_preds, gbm = XGB_pipeline.bag_run(X_train, y_train=train[target_columns],\n",
    "                                           #X_valid=X_valid, y_valid=X_valid[target_columns],\n",
    "                                           X_test=X_test,\n",
    "                                           model_params=gbm_params,\n",
    "                                           train_params=train_params,\n",
    "                                           output_submission=True)\n",
    "else:\n",
    "    val_preds, gbm = XGB_pipeline.bag_run(X_train, y_train=X_train[targets],\n",
    "                               model_params=gbm_params,\n",
    "                               train_params=train_params)\n",
    "\n",
    "\n",
    "utils.save_parameter_dict(\n",
    "    'checkpoints/{0}/{0}_gbm_parameters.txt'.format(pipeline_params['run_save_name']), gbm_params)\n",
    "utils.save_parameter_dict('checkpoints/{0}/{0}_train_parameters.txt'.format(\n",
    "    pipeline_params['run_save_name']), train_params)\n",
    "utils.save_parameter_dict('checkpoints/{0}/{0}_pipeline_parameters.txt'.format(\n",
    "    pipeline_params['run_save_name']), pipeline_params)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "if pipeline_params['use_lgb']:\n",
    "    gbm_params = lgb_params\n",
    "else:\n",
    "    gbm_params = xgb_params\n",
    "\n",
    "\n",
    "if pipeline_params['predict_test']:\n",
    "    val_preds, test_preds, gbm = XGB_pipeline.fold_run(X_train, y_train=train[target_columns],\n",
    "                                           X_test=X_test,\n",
    "                                           model_params=gbm_params,\n",
    "                                           train_params=train_params,\n",
    "                                           output_submission=True)\n",
    "else:\n",
    "    val_preds, gbm = XGB_pipeline.fold_run(X_train, y_train=X_train[targets],\n",
    "                               model_params=gbm_params,\n",
    "                               train_params=train_params)\n",
    "\n",
    "\n",
    "utils.save_parameter_dict(\n",
    "    'checkpoints/{0}/{0}_gbm_parameters.txt'.format(pipeline_params['run_save_name']), gbm_params)\n",
    "utils.save_parameter_dict('checkpoints/{0}/{0}_train_parameters.txt'.format(\n",
    "    pipeline_params['run_save_name']), train_params)\n",
    "utils.save_parameter_dict('checkpoints/{0}/{0}_pipeline_parameters.txt'.format(\n",
    "    pipeline_params['run_save_name']), pipeline_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
