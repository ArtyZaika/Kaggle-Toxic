{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras_models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import SGD, Adadelta, Adam, Nadam, RMSprop\n",
    "from keras.preprocessing import sequence, text\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: MLPbasicFastText_1bag_BS256_Nadam_SpacyClean\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "n_bags = 1\n",
    "split_size = 0.1\n",
    "max_features = 300000\n",
    "nb_words = max_features\n",
    "sequence_length = 196\n",
    "embedding_dim = 300\n",
    "bidirectional = True\n",
    "run_prefix = '256_FastText300k_'\n",
    "embedding_filename = 'FastText_300dim_embeddingBasic300k'\n",
    "\n",
    "run_prefix = 'FastText_'\n",
    "src = '/home/w/Projects/Toxic/data/'\n",
    "model_name = 'MLPbasic'\n",
    "optimizer = 'Nadam'\n",
    "data_type = 'SpacyClean'\n",
    "kfold_run = 0\n",
    "batch_size = 256\n",
    "importance = 0\n",
    "stratify = 0\n",
    "save_models = 0\n",
    "load_models = 0\n",
    "save_oof = 0\n",
    "prepare_submission = 1\n",
    "\n",
    "\n",
    "if bidirectional and 'LSTM' in model_name or bidirectional and 'GRU' in model_name:\n",
    "    run_prefix = 'Bidirectional{}'.format(run_prefix)\n",
    "if kfold_run:\n",
    "    general_run_name = '{}{}fold_BS{}_{}'.format(\n",
    "        run_prefix, n_folds, batch_size, optimizer)\n",
    "else:\n",
    "    general_run_name = '{}{}bag_BS{}_{}'.format(\n",
    "        run_prefix, n_bags, batch_size, optimizer)\n",
    "\n",
    "\n",
    "if len(data_type) > 0:\n",
    "    general_run_name += '_{}'.format(data_type)\n",
    "if importance:\n",
    "    general_run_name += '_ImportanceTrain'\n",
    "if stratify and kfold_run:\n",
    "    general_run_name += '_Stratified'\n",
    "\n",
    "run_name = '{}{}'.format(model_name, general_run_name)\n",
    "print('Run name: {}'.format(run_name))\n",
    "\n",
    "\n",
    "model_callbacks = [EarlyStopping(monitor='val_loss', patience=18, verbose=1),\n",
    "                   ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1,\n",
    "                                     patience=8, min_lr=1e-5)]\n",
    "\n",
    "if optimizer == 'Adam':\n",
    "    optimizer = Adam(lr=1e-4, decay=1e-3)\n",
    "    # optimizer = 'adam'\n",
    "if optimizer == 'Nadam':\n",
    "    optimizer = Nadam(lr=1e-4, schedule_decay=1e-3)\n",
    "    # optimizer = 'nadam'\n",
    "if optimizer == 'SGD':\n",
    "    optimizer = SGD(lr=1e-3, momentum=0.9,\n",
    "                    decay=1e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_features = '/home/w/Projects/Toxic/data/features/'\n",
    "run_kfold = True\n",
    "split_for_validation = True\n",
    "\n",
    "if run_kfold:\n",
    "    prefix = 'KFold'\n",
    "else:\n",
    "    prefix = 'Bag'\n",
    "\n",
    "\n",
    "train = pd.read_pickle(\"../data/train_basic_clean.pkl\")\n",
    "test = pd.read_pickle(\"../data/test_basic_clean.pkl\")\n",
    "target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "data_tokenized = pd.read_pickle(src_features + 'data_TokenizedSentences196.pkl')\n",
    "data_badwords300 = pd.read_pickle(src_features + 'data_Binary300Badwords.pkl')\n",
    "data_badwordsCount = pd.read_pickle(src_features + 'data_BadwordsCount.pkl')\n",
    "data_textStatistics = pd.read_pickle(src_features + 'data_TextStatistics.pkl')\n",
    "data_transformations = pd.read_pickle(src_features + 'data_TransformationsFeats20dim_SVDLSA.pkl')\n",
    "\n",
    "X = pd.concat([data_tokenized, data_badwords300, data_textStatistics, data_transformations], axis=1)\n",
    "X['badwordsCount'] = data_badwordsCount\n",
    "\n",
    "X_train = X.iloc[:train.shape[0], :]\n",
    "X_test = X.iloc[train.shape[0]:, :]\n",
    "y_train = train[target_columns]\n",
    "\n",
    "features = np.setdiff1d(X_train.columns, target_columns)\n",
    "\n",
    "\n",
    "\n",
    "del X, test\n",
    "del data_tokenized, data_badwords300, data_badwordsCount\n",
    "del data_textStatistics, data_transformations\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running parametrized bagging\n",
      "Running: MLPbasicFastText_1bag_BS256_Nadam_SpacyClean\n",
      "Training on bag: 1 \n",
      "\n",
      "Saving CSV logs for model from current bag/fold: MLPbasicFastText_1bag_BS256_Nadam_SpacyClean, bag number 1 \n",
      "\n",
      "Splitting data - validation split size: 0.1, split seed: 1337\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/1000\n",
      "86265/86265 [==============================] - 3s 40us/step - loss: 0.2176 - acc: 0.9273 - val_loss: 0.0942 - val_acc: 0.9733\n",
      "Epoch 2/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0833 - acc: 0.9759 - val_loss: 0.0699 - val_acc: 0.9787\n",
      "Epoch 3/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0758 - acc: 0.9770 - val_loss: 0.0674 - val_acc: 0.9795\n",
      "Epoch 4/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0726 - acc: 0.9775 - val_loss: 0.0660 - val_acc: 0.9797\n",
      "Epoch 5/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0711 - acc: 0.9780 - val_loss: 0.0660 - val_acc: 0.9797\n",
      "Epoch 6/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0705 - acc: 0.9780 - val_loss: 0.0648 - val_acc: 0.9798\n",
      "Epoch 7/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0692 - acc: 0.9785 - val_loss: 0.0645 - val_acc: 0.9799\n",
      "Epoch 8/1000\n",
      "86265/86265 [==============================] - 3s 33us/step - loss: 0.0682 - acc: 0.9785 - val_loss: 0.0644 - val_acc: 0.9800\n",
      "Epoch 9/1000\n",
      "86265/86265 [==============================] - 3s 33us/step - loss: 0.0669 - acc: 0.9787 - val_loss: 0.0641 - val_acc: 0.9796\n",
      "Epoch 10/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0671 - acc: 0.9788 - val_loss: 0.0642 - val_acc: 0.9797\n",
      "Epoch 11/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0661 - acc: 0.9791 - val_loss: 0.0639 - val_acc: 0.9800\n",
      "Epoch 12/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0666 - acc: 0.9789 - val_loss: 0.0638 - val_acc: 0.9798\n",
      "Epoch 13/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0655 - acc: 0.9792 - val_loss: 0.0634 - val_acc: 0.9799\n",
      "Epoch 14/1000\n",
      "86265/86265 [==============================] - 3s 33us/step - loss: 0.0656 - acc: 0.9791 - val_loss: 0.0635 - val_acc: 0.9794\n",
      "Epoch 15/1000\n",
      "86265/86265 [==============================] - 3s 34us/step - loss: 0.0648 - acc: 0.9793 - val_loss: 0.0635 - val_acc: 0.9798\n",
      "Epoch 16/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0644 - acc: 0.9794 - val_loss: 0.0631 - val_acc: 0.9798\n",
      "Epoch 17/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0645 - acc: 0.9796 - val_loss: 0.0630 - val_acc: 0.9796\n",
      "Epoch 18/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0638 - acc: 0.9796 - val_loss: 0.0628 - val_acc: 0.9799\n",
      "Epoch 19/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0630 - acc: 0.9798 - val_loss: 0.0633 - val_acc: 0.9798\n",
      "Epoch 20/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0635 - acc: 0.9797 - val_loss: 0.0634 - val_acc: 0.9797\n",
      "Epoch 21/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0622 - acc: 0.9800 - val_loss: 0.0630 - val_acc: 0.9799\n",
      "Epoch 22/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0626 - acc: 0.9799 - val_loss: 0.0629 - val_acc: 0.9799\n",
      "Epoch 23/1000\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0625 - acc: 0.9800 - val_loss: 0.0629 - val_acc: 0.9799\n",
      "Epoch 24/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0621 - acc: 0.9802 - val_loss: 0.0637 - val_acc: 0.9795\n",
      "Epoch 25/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0614 - acc: 0.9802 - val_loss: 0.0633 - val_acc: 0.9798\n",
      "Epoch 26/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0614 - acc: 0.9802 - val_loss: 0.0632 - val_acc: 0.9795\n",
      "Epoch 27/1000\n",
      "84480/86265 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9804\n",
      "Epoch 00027: reducing learning rate to 4.999999873689376e-05.\n",
      "86265/86265 [==============================] - 3s 30us/step - loss: 0.0604 - acc: 0.9804 - val_loss: 0.0632 - val_acc: 0.9796\n",
      "Epoch 28/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0606 - acc: 0.9805 - val_loss: 0.0630 - val_acc: 0.9795\n",
      "Epoch 29/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0608 - acc: 0.9804 - val_loss: 0.0634 - val_acc: 0.9794\n",
      "Epoch 30/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0605 - acc: 0.9805 - val_loss: 0.0632 - val_acc: 0.9794\n",
      "Epoch 31/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0606 - acc: 0.9804 - val_loss: 0.0629 - val_acc: 0.9797\n",
      "Epoch 32/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0603 - acc: 0.9806 - val_loss: 0.0631 - val_acc: 0.9795\n",
      "Epoch 33/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0599 - acc: 0.9805 - val_loss: 0.0630 - val_acc: 0.9797\n",
      "Epoch 34/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0600 - acc: 0.9805 - val_loss: 0.0631 - val_acc: 0.9796\n",
      "Epoch 35/1000\n",
      "85504/86265 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9805\n",
      "Epoch 00035: reducing learning rate to 2.499999936844688e-05.\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0602 - acc: 0.9804 - val_loss: 0.0629 - val_acc: 0.9796\n",
      "Epoch 36/1000\n",
      "86265/86265 [==============================] - 3s 29us/step - loss: 0.0596 - acc: 0.9807 - val_loss: 0.0631 - val_acc: 0.9797\n",
      "Epoch 00036: early stopping\n",
      "Loss statistics for best epoch in current run: \n",
      " Mean: 0.06277964500278614 \n",
      " Minimum: 0.06277964500278614 \n",
      " Maximum: 0.06277964500278614 \n",
      " Standard Deviation: 0.0 \n",
      "\n",
      "Predicting on validation data.\n",
      "Validation split - standard deviation for original target values: toxic            0.289480\n",
      "severe_toxic     0.099571\n",
      "obscene          0.221928\n",
      "threat           0.056775\n",
      "insult           0.211741\n",
      "identity_hate    0.088687\n",
      "dtype: float64 \n",
      "                  for predicted target values: 0.1400623470544815 \n",
      " \n",
      "\n",
      "Predicting on test data.\n",
      "(1, 9586, 6) (1, 226998, 6)\n"
     ]
    }
   ],
   "source": [
    "model_parameters = {\n",
    "    'lstm_units': 256,\n",
    "    'bidirectional': bidirectional,\n",
    "    'nb_words': nb_words,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    #'embedding_matrix': embedding_matrix,\n",
    "    'sequence_length': sequence_length,\n",
    "    'optimizer': optimizer,\n",
    "    'num_columns': X_train.shape[1],\n",
    "}\n",
    "\n",
    "pipeline_parameters = {\n",
    "    'model_name': getattr(keras_models, model_name),\n",
    "    'predict_test': True,\n",
    "    'number_epochs': 1000,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': 1337,\n",
    "    'shuffle': True,\n",
    "    'verbose': True,\n",
    "    'run_save_name': run_name,\n",
    "    'load_keras_model': load_models,\n",
    "    'save_model': save_models,\n",
    "    'save_history': True,\n",
    "    'save_statistics': True,\n",
    "    'output_statistics': True,\n",
    "    'src_dir': os.getcwd(),\n",
    "}\n",
    "\n",
    "if kfold_run:\n",
    "    oof_train, oof_test = utils.run_parametrized_kfold(X_train[features], y_train, \n",
    "                                                       X_test[features],\n",
    "                                                       pipeline_parameters,\n",
    "                                                       model_parameters,\n",
    "                                                       model_callbacks=model_callbacks,\n",
    "                                                       n_folds=n_folds,\n",
    "                                                       importance_training=importance,\n",
    "                                                       save_oof=save_oof)\n",
    "    print(oof_train.shape, oof_test.shape)\n",
    "else:\n",
    "    oof_valid, oof_test = utils.run_parametrized_bagging(X_train[features], y_train,\n",
    "                                                         X_test=X_test[features],\n",
    "                                                         pipeline_parameters=pipeline_parameters,\n",
    "                                                         model_parameters=model_parameters,\n",
    "                                                         model_callbacks=model_callbacks,\n",
    "                                                         n_bags=n_bags,\n",
    "                                                         split_size=split_size,\n",
    "                                                         importance_training=importance)\n",
    "    print(oof_valid.shape, oof_test.shape)\n",
    "\n",
    "\n",
    "if prepare_submission:\n",
    "    submission = utils.output_submission(\n",
    "        oof_test.mean(axis=0), run_name, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
