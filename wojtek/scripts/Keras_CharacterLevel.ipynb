{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import keras_models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import SGD, Adadelta, Adam, Nadam, RMSprop\n",
    "from keras.preprocessing import sequence, text\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: LSTMattentionCharFastText_1bag_BS256_Nadam_BasicClean\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "n_bags = 1\n",
    "split_size = 0.1\n",
    "max_features = 300000\n",
    "nb_words = max_features\n",
    "sequence_length = 1024\n",
    "embedding_dim = 300\n",
    "bidirectional = False\n",
    "run_prefix = '1024len_Glove300k_CharLevel1_'\n",
    "embedding_filename = 'Glove_300dim_embeddingBasic300k'\n",
    "\n",
    "run_prefix = 'FastText_'\n",
    "src = '/home/w/Projects/Toxic/data/'\n",
    "model_name = 'LSTMattentionChar'\n",
    "optimizer = 'Nadam'\n",
    "data_type = 'BasicClean'\n",
    "kfold_run = 0\n",
    "batch_size = 256\n",
    "importance = 0\n",
    "stratify = 0\n",
    "save_models = 0\n",
    "load_models = 0\n",
    "save_oof = 0\n",
    "prepare_submission = 1\n",
    "\n",
    "\n",
    "if bidirectional and 'LSTM' in model_name or bidirectional and 'GRU' in model_name:\n",
    "    run_prefix = 'Bidirectional{}'.format(run_prefix)\n",
    "if kfold_run:\n",
    "    general_run_name = '{}{}fold_BS{}_{}'.format(\n",
    "        run_prefix, n_folds, batch_size, optimizer)\n",
    "else:\n",
    "    general_run_name = '{}{}bag_BS{}_{}'.format(\n",
    "        run_prefix, n_bags, batch_size, optimizer)\n",
    "\n",
    "\n",
    "if len(data_type) > 0:\n",
    "    general_run_name += '_{}'.format(data_type)\n",
    "if importance:\n",
    "    general_run_name += '_ImportanceTrain'\n",
    "if stratify and kfold_run:\n",
    "    general_run_name += '_Stratified'\n",
    "\n",
    "run_name = '{}{}'.format(model_name, general_run_name)\n",
    "print('Run name: {}'.format(run_name))\n",
    "\n",
    "\n",
    "model_callbacks = [EarlyStopping(monitor='val_loss', patience=18, verbose=1),\n",
    "                   ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1,\n",
    "                                     patience=8, min_lr=1e-5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data with basic cleaning.\n",
      "(95851, 8) (226998, 2)\n",
      "(95851, 1024) (95851, 6) (226998, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = utils.load_data(src, mode=data_type)\n",
    "print(train.shape, test.shape)\n",
    "list_classes = [\"toxic\", \"severe_toxic\",\n",
    "                \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features, char_level=True)\n",
    "tokenizer.fit_on_texts(train.comment_text.tolist() + test.comment_text.tolist())\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index)) + 1\n",
    "\n",
    "X_train = sequence.pad_sequences(\n",
    "    list_tokenized_train, maxlen=sequence_length)  # [:1000]\n",
    "y_train = train[list_classes].values  # [:1000]\n",
    "X_test = sequence.pad_sequences(\n",
    "    list_tokenized_test, maxlen=sequence_length)  # [:1000]\n",
    "print(X_train.shape, y_train.shape, X_test.shape)\n",
    "\n",
    "del train, test, list_tokenized_train, list_tokenized_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from keras.layers import (GRU, LSTM, Bidirectional, CuDNNGRU, CuDNNLSTM, Dense,\n",
    "                          Dropout, Embedding, Flatten, Input, Lambda, Reshape,\n",
    "                          concatenate)\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.convolutional import (AveragePooling1D, Conv1D, MaxPooling1D,\n",
    "                                        ZeroPadding1D)\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras_attention import Attention\n",
    "from keras_attention_context import AttentionWithContext\n",
    "\n",
    "\n",
    "\n",
    "def CharacterlevelCNN(conv_layers = 2, \n",
    "                dilation_rates = [0, 2, 4, 8, 16], \n",
    "                embed_size = 256):\n",
    "    inp = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer.word_counts)+1, \n",
    "                  output_dim = embed_size)(inp)\n",
    "    prefilt_x = Dropout(0.25)(x)\n",
    "    out_conv = []\n",
    "    # dilation rate lets us use ngrams and skip grams to process \n",
    "    for dilation_rate in dilation_rates:\n",
    "        x = prefilt_x\n",
    "        for i in range(2):\n",
    "            if dilation_rate>0:\n",
    "                x = Conv1D(16*2**(i), \n",
    "                           kernel_size = 3, \n",
    "                           dilation_rate = dilation_rate,\n",
    "                          activation = 'relu',\n",
    "                          name = 'ngram_{}_cnn_{}'.format(dilation_rate, i)\n",
    "                          )(x)\n",
    "            else:\n",
    "                x = Conv1D(16*2**(i), \n",
    "                           kernel_size = 1,\n",
    "                          activation = 'relu',\n",
    "                          name = 'word_fcl_{}'.format(i))(x)\n",
    "        out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    x = concatenate(out_conv, axis = -1)    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def LSTMattentionChar(params):\n",
    "\n",
    "    Embedding_layer = Embedding(params['nb_words'],\n",
    "                                params['embedding_dim'],\n",
    "                                input_length=params['sequence_length'],\n",
    "                                trainable=True)\n",
    "\n",
    "    input_ = Input(shape=(params['sequence_length'], ))\n",
    "    embed_input_ = Embedding_layer(input_)\n",
    "\n",
    "    if params['bidirectional']:\n",
    "        x = Bidirectional(\n",
    "            CuDNNLSTM(params['lstm_units'], return_sequences=True))(embed_input_)\n",
    "    else:\n",
    "        x = CuDNNLSTM(params['lstm_units'],\n",
    "                      return_sequences=True)(embed_input_)\n",
    "    x = AttentionWithContext()(x)\n",
    "    # x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "if optimizer == 'Adam':\n",
    "    optimizer = Adam(lr=1e-4, decay=1e-3)\n",
    "    # optimizer = 'adam'\n",
    "if optimizer == 'Nadam':\n",
    "    optimizer = Nadam(lr=1e-4, schedule_decay=1e-3)\n",
    "    # optimizer = 'nadam'\n",
    "if optimizer == 'SGD':\n",
    "    optimizer = SGD(lr=1e-3, momentum=0.9,\n",
    "                    decay=1e-4, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running parametrized bagging\n",
      "Running: LSTMattentionCharFastText_1bag_BS256_Nadam_BasicClean\n",
      "Training on bag: 1 \n",
      "\n",
      "Saving CSV logs for model from current bag/fold: LSTMattentionCharFastText_1bag_BS256_Nadam_BasicClean, bag number 1 \n",
      "\n",
      "Splitting data - validation split size: 0.1, split seed: 1337\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/1000\n",
      "86265/86265 [==============================] - 122s 1ms/step - loss: 0.1772 - acc: 0.9601 - val_loss: 0.1377 - val_acc: 0.9646\n",
      "Epoch 2/1000\n",
      "86265/86265 [==============================] - 123s 1ms/step - loss: 0.1430 - acc: 0.9630 - val_loss: 0.1346 - val_acc: 0.9646\n",
      "Epoch 3/1000\n",
      "86265/86265 [==============================] - 124s 1ms/step - loss: 0.1338 - acc: 0.9630 - val_loss: 0.1290 - val_acc: 0.9646\n",
      "Epoch 4/1000\n",
      "86265/86265 [==============================] - 124s 1ms/step - loss: 0.1222 - acc: 0.9634 - val_loss: 0.1075 - val_acc: 0.9680\n",
      "Epoch 5/1000\n",
      "86265/86265 [==============================] - 124s 1ms/step - loss: 0.1018 - acc: 0.9705 - val_loss: 0.0924 - val_acc: 0.9738\n",
      "Epoch 6/1000\n",
      "86265/86265 [==============================] - 124s 1ms/step - loss: 0.0943 - acc: 0.9737 - val_loss: 0.0907 - val_acc: 0.9746\n",
      "Epoch 7/1000\n",
      "86265/86265 [==============================] - 124s 1ms/step - loss: 0.0921 - acc: 0.9742 - val_loss: 0.0884 - val_acc: 0.9747\n",
      "Epoch 8/1000\n",
      "86265/86265 [==============================] - 125s 1ms/step - loss: 0.0908 - acc: 0.9744 - val_loss: 0.0878 - val_acc: 0.9746\n",
      "Epoch 9/1000\n",
      "86265/86265 [==============================] - 125s 1ms/step - loss: 0.0899 - acc: 0.9746 - val_loss: 0.0865 - val_acc: 0.9757\n",
      "Epoch 10/1000\n",
      "86265/86265 [==============================] - 125s 1ms/step - loss: 0.0891 - acc: 0.9747 - val_loss: 0.0850 - val_acc: 0.9759\n",
      "Epoch 11/1000\n",
      "19456/86265 [=====>........................] - ETA: 1:33 - loss: 0.0921 - acc: 0.9737"
     ]
    }
   ],
   "source": [
    "model_parameters = {\n",
    "    'lstm_units': 256,\n",
    "    'bidirectional': bidirectional,\n",
    "    'nb_words': len(tokenizer.word_counts) + 1,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'sequence_length': sequence_length,\n",
    "    'optimizer': optimizer,\n",
    "    'num_columns': X_train.shape[1],\n",
    "}\n",
    "\n",
    "pipeline_parameters = {\n",
    "    'model_name': LSTMattentionChar,\n",
    "    'predict_test': True,\n",
    "    'number_epochs': 1000,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': 1337,\n",
    "    'shuffle': True,\n",
    "    'verbose': True,\n",
    "    'run_save_name': run_name,\n",
    "    'load_keras_model': load_models,\n",
    "    'save_model': save_models,\n",
    "    'save_history': True,\n",
    "    'save_statistics': True,\n",
    "    'output_statistics': True,\n",
    "    'src_dir': os.getcwd(),\n",
    "}\n",
    "\n",
    "if kfold_run:\n",
    "    oof_train, oof_test = utils.run_parametrized_kfold(X_train[features], y_train, \n",
    "                                                       X_test[features],\n",
    "                                                       pipeline_parameters,\n",
    "                                                       model_parameters,\n",
    "                                                       model_callbacks=model_callbacks,\n",
    "                                                       n_folds=n_folds,\n",
    "                                                       importance_training=importance,\n",
    "                                                       save_oof=save_oof)\n",
    "    print(oof_train.shape, oof_test.shape)\n",
    "else:\n",
    "    oof_valid, oof_test = utils.run_parametrized_bagging(X_train, y_train,\n",
    "                                                         X_test=X_test,\n",
    "                                                         pipeline_parameters=pipeline_parameters,\n",
    "                                                         model_parameters=model_parameters,\n",
    "                                                         model_callbacks=model_callbacks,\n",
    "                                                         n_bags=n_bags,\n",
    "                                                         split_size=split_size,\n",
    "                                                         importance_training=importance)\n",
    "    print(oof_valid.shape, oof_test.shape)\n",
    "\n",
    "\n",
    "if prepare_submission:\n",
    "    submission = utils.output_submission(\n",
    "        oof_test.mean(axis=0), run_name, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
